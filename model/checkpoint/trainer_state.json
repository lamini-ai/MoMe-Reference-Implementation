{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.09,
  "eval_steps": 20,
  "global_step": 100,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01,
      "flops": 0.0,
      "grad_norm": 2.6109461784362793,
      "iter_time": 0.0,
      "learning_rate": 9e-05,
      "loss": 10.3954,
      "remaining_time": 0.0,
      "step": 1
    },
    {
      "epoch": 0.02,
      "flops": 2030717245.8103936,
      "grad_norm": 2.609140157699585,
      "iter_time": 44.00721001625061,
      "learning_rate": 0.00018,
      "loss": 10.3954,
      "remaining_time": 4312.70658159256,
      "step": 2
    },
    {
      "epoch": 0.03,
      "flops": 2067376357.9977539,
      "grad_norm": 2.627166748046875,
      "iter_time": 43.22686576843262,
      "learning_rate": 0.00027,
      "loss": 10.379,
      "remaining_time": 4193.005979537964,
      "step": 3
    },
    {
      "epoch": 0.04,
      "flops": 2081668432.5958214,
      "grad_norm": 2.5155935287475586,
      "iter_time": 42.93008383115133,
      "learning_rate": 0.00036,
      "loss": 10.3419,
      "remaining_time": 4121.288047790527,
      "step": 4
    },
    {
      "epoch": 0.05,
      "flops": 2064652431.5088103,
      "grad_norm": 2.22637939453125,
      "iter_time": 43.283895611763,
      "learning_rate": 0.00045,
      "loss": 10.2943,
      "remaining_time": 4111.970083117485,
      "step": 5
    },
    {
      "epoch": 0.06,
      "flops": 2067522906.1544497,
      "grad_norm": 1.7676602602005005,
      "iter_time": 43.22380180358887,
      "learning_rate": 0.00054,
      "loss": 10.2375,
      "remaining_time": 4063.0373695373537,
      "step": 6
    },
    {
      "epoch": 0.07,
      "flops": 2062695957.276464,
      "grad_norm": 1.3206229209899902,
      "iter_time": 43.324950536092125,
      "learning_rate": 0.0006299999999999999,
      "loss": 10.1801,
      "remaining_time": 4029.2203998565674,
      "step": 7
    },
    {
      "epoch": 0.08,
      "flops": 2041232614.1058433,
      "grad_norm": 1.879279375076294,
      "iter_time": 43.78050776890346,
      "learning_rate": 0.00072,
      "loss": 10.1322,
      "remaining_time": 4027.8067147391184,
      "step": 8
    },
    {
      "epoch": 0.09,
      "flops": 2043080564.0065815,
      "grad_norm": 2.663644552230835,
      "iter_time": 43.74090865254402,
      "learning_rate": 0.00081,
      "loss": 10.1029,
      "remaining_time": 3980.422687381506,
      "step": 9
    },
    {
      "epoch": 0.1,
      "flops": 2038534604.5219688,
      "grad_norm": 2.6555399894714355,
      "iter_time": 43.83845146497091,
      "learning_rate": 0.0009,
      "loss": 10.0497,
      "remaining_time": 3945.4606318473816,
      "step": 10
    },
    {
      "epoch": 1.01,
      "flops": 2040396918.6051502,
      "grad_norm": 2.51346755027771,
      "iter_time": 43.79843916893005,
      "learning_rate": 0.0008900000000000001,
      "loss": 9.9803,
      "remaining_time": 3898.0610860347747,
      "step": 11
    },
    {
      "epoch": 1.02,
      "flops": 2042675705.3619754,
      "grad_norm": 2.2082414627075195,
      "iter_time": 43.749578107487075,
      "learning_rate": 0.0008799999999999999,
      "loss": 9.8966,
      "remaining_time": 3849.9628734588628,
      "step": 12
    },
    {
      "epoch": 1.03,
      "flops": 2046207404.0840273,
      "grad_norm": 2.3158249855041504,
      "iter_time": 43.674067517121635,
      "learning_rate": 0.00087,
      "loss": 9.8019,
      "remaining_time": 3799.643873989582,
      "step": 13
    },
    {
      "epoch": 1.04,
      "flops": 2047077081.5658925,
      "grad_norm": 2.329846143722534,
      "iter_time": 43.655513084851776,
      "learning_rate": 0.00086,
      "loss": 9.6985,
      "remaining_time": 3754.3741252972527,
      "step": 14
    },
    {
      "epoch": 1.05,
      "flops": 2045401966.9900417,
      "grad_norm": 2.795389413833618,
      "iter_time": 43.69126546382904,
      "learning_rate": 0.00085,
      "loss": 9.6039,
      "remaining_time": 3713.7575644254684,
      "step": 15
    },
    {
      "epoch": 1.06,
      "flops": 2051402233.093179,
      "grad_norm": 2.251502752304077,
      "iter_time": 43.5634703318278,
      "learning_rate": 0.00084,
      "loss": 9.4868,
      "remaining_time": 3659.3315078735354,
      "step": 16
    },
    {
      "epoch": 1.07,
      "flops": 2049473767.7769306,
      "grad_norm": 2.5767810344696045,
      "iter_time": 43.604461655020714,
      "learning_rate": 0.00083,
      "loss": 9.3826,
      "remaining_time": 3619.1703173667192,
      "step": 17
    },
    {
      "epoch": 1.08,
      "flops": 2037996705.2697728,
      "grad_norm": 2.258354663848877,
      "iter_time": 43.85002197938807,
      "learning_rate": 0.00082,
      "loss": 9.2563,
      "remaining_time": 3595.7018023098217,
      "step": 18
    },
    {
      "epoch": 1.09,
      "flops": 2039151703.0397398,
      "grad_norm": 2.1316802501678467,
      "iter_time": 43.82518484857347,
      "learning_rate": 0.00081,
      "loss": 9.1383,
      "remaining_time": 3549.8399727344513,
      "step": 19
    },
    {
      "epoch": 1.1,
      "flops": 2035328263.812905,
      "grad_norm": 2.542773485183716,
      "iter_time": 43.907512075022645,
      "learning_rate": 0.0007999999999999999,
      "loss": 9.0183,
      "remaining_time": 3512.6009660018117,
      "step": 20
    },
    {
      "epoch": 2.01,
      "flops": 2035089361.79234,
      "grad_norm": 2.371464490890503,
      "iter_time": 43.91266644001007,
      "learning_rate": 0.00079,
      "loss": 8.891,
      "remaining_time": 3469.1006487607956,
      "step": 21
    },
    {
      "epoch": 2.02,
      "flops": 2037800962.7952642,
      "grad_norm": 2.3785088062286377,
      "iter_time": 43.85423402559189,
      "learning_rate": 0.00078,
      "loss": 8.7575,
      "remaining_time": 3420.6302539961675,
      "step": 22
    },
    {
      "epoch": 2.03,
      "flops": 2042177784.1744547,
      "grad_norm": 2.3470280170440674,
      "iter_time": 43.76024507392537,
      "learning_rate": 0.00077,
      "loss": 8.6279,
      "remaining_time": 3369.5388706922536,
      "step": 23
    },
    {
      "epoch": 2.04,
      "flops": 2044279783.2191677,
      "grad_norm": 2.2364754676818848,
      "iter_time": 43.715249279271,
      "learning_rate": 0.0007599999999999999,
      "loss": 8.4887,
      "remaining_time": 3322.358945224596,
      "step": 24
    },
    {
      "epoch": 2.05,
      "flops": 2047284999.8596117,
      "grad_norm": 2.300999879837036,
      "iter_time": 43.6510795156161,
      "learning_rate": 0.00075,
      "loss": 8.3574,
      "remaining_time": 3273.8309636712074,
      "step": 25
    },
    {
      "epoch": 2.06,
      "flops": 2046873091.8439162,
      "grad_norm": 2.3121795654296875,
      "iter_time": 43.659863758087155,
      "learning_rate": 0.00074,
      "loss": 8.2184,
      "remaining_time": 3230.8299180984495,
      "step": 26
    },
    {
      "epoch": 2.07,
      "flops": 277534522.0547856,
      "grad_norm": 2.1062982082366943,
      "iter_time": 322.00030345183154,
      "learning_rate": 0.00073,
      "loss": 8.0802,
      "remaining_time": 23506.022151983703,
      "step": 27
    },
    {
      "epoch": 2.08,
      "flops": 108742988.11475694,
      "grad_norm": 3.4672718048095703,
      "iter_time": 821.8111518665596,
      "learning_rate": 0.00072,
      "loss": 7.9467,
      "remaining_time": 59170.40293439229,
      "step": 28
    },
    {
      "epoch": 0.01,
      "flops": 205836968.47625822,
      "grad_norm": 27.8189697265625,
      "iter_time": 434.1601073001993,
      "learning_rate": 0.0007099999999999999,
      "loss": 8.5819,
      "remaining_time": 30825.36761831415,
      "step": 29
    },
    {
      "epoch": 0.02,
      "flops": 212182043.61097842,
      "grad_norm": 10.823575973510742,
      "iter_time": 421.17701761722566,
      "learning_rate": 0.0007,
      "loss": 8.0929,
      "remaining_time": 29482.391233205795,
      "step": 30
    },
    {
      "epoch": 0.03,
      "flops": 218492234.35257533,
      "grad_norm": 4.115504264831543,
      "iter_time": 409.0131650893919,
      "learning_rate": 0.0006900000000000001,
      "loss": 7.8165,
      "remaining_time": 28221.908391168043,
      "step": 31
    },
    {
      "epoch": 0.04,
      "flops": 224768420.65057945,
      "grad_norm": 2.0131402015686035,
      "iter_time": 397.5923310816288,
      "learning_rate": 0.0006799999999999999,
      "loss": 7.6588,
      "remaining_time": 27036.27851355076,
      "step": 32
    },
    {
      "epoch": 0.05,
      "flops": 230982057.94882104,
      "grad_norm": 2.4269628524780273,
      "iter_time": 386.8967187910369,
      "learning_rate": 0.00067,
      "loss": 7.5278,
      "remaining_time": 25922.080158999473,
      "step": 33
    },
    {
      "epoch": 0.06,
      "flops": 237189420.76534238,
      "grad_norm": 2.929123878479004,
      "iter_time": 376.771442974315,
      "learning_rate": 0.0006599999999999999,
      "loss": 7.4027,
      "remaining_time": 24866.91523630479,
      "step": 34
    },
    {
      "epoch": 0.07,
      "flops": 243387879.74111807,
      "grad_norm": 2.453481674194336,
      "iter_time": 367.1760500771659,
      "learning_rate": 0.00065,
      "loss": 7.2609,
      "remaining_time": 23866.443255015784,
      "step": 35
    },
    {
      "epoch": 0.08,
      "flops": 249522463.88085824,
      "grad_norm": 2.7013418674468994,
      "iter_time": 358.1489174564679,
      "learning_rate": 0.00064,
      "loss": 7.13,
      "remaining_time": 22921.530717213947,
      "step": 36
    },
    {
      "epoch": 0.09,
      "flops": 255640388.38440102,
      "grad_norm": 2.6736326217651367,
      "iter_time": 349.57778340416985,
      "learning_rate": 0.0006299999999999999,
      "loss": 7.0032,
      "remaining_time": 22023.4003544627,
      "step": 37
    },
    {
      "epoch": 0.1,
      "flops": 261681928.51466072,
      "grad_norm": 2.5931038856506348,
      "iter_time": 341.5069616280104,
      "learning_rate": 0.00062,
      "loss": 6.8748,
      "remaining_time": 21173.431620936644,
      "step": 38
    },
    {
      "epoch": 1.01,
      "flops": 267685475.2928447,
      "grad_norm": 2.5000176429748535,
      "iter_time": 333.8477749763391,
      "learning_rate": 0.00061,
      "loss": 6.7478,
      "remaining_time": 20364.714273556685,
      "step": 39
    },
    {
      "epoch": 1.02,
      "flops": 273671427.61337537,
      "grad_norm": 2.3970773220062256,
      "iter_time": 326.5455992221832,
      "learning_rate": 0.0006,
      "loss": 6.6199,
      "remaining_time": 19592.735953330994,
      "step": 40
    },
    {
      "epoch": 1.03,
      "flops": 279600642.11908615,
      "grad_norm": 2.366389036178589,
      "iter_time": 319.62086940393215,
      "learning_rate": 0.00059,
      "loss": 6.4992,
      "remaining_time": 18857.631294831997,
      "step": 41
    },
    {
      "epoch": 1.04,
      "flops": 285509186.3529467,
      "grad_norm": 2.32491135597229,
      "iter_time": 313.00639205887205,
      "learning_rate": 0.00058,
      "loss": 6.3841,
      "remaining_time": 18154.37073941458,
      "step": 42
    },
    {
      "epoch": 1.05,
      "flops": 291349581.2264325,
      "grad_norm": 2.269195079803467,
      "iter_time": 306.73186466860216,
      "learning_rate": 0.00057,
      "loss": 6.2745,
      "remaining_time": 17483.716286110324,
      "step": 43
    },
    {
      "epoch": 1.06,
      "flops": 297183353.74240935,
      "grad_norm": 2.168900489807129,
      "iter_time": 300.71065284989095,
      "learning_rate": 0.00056,
      "loss": 6.1641,
      "remaining_time": 16839.796559593895,
      "step": 44
    },
    {
      "epoch": 1.07,
      "flops": 302982987.9041403,
      "grad_norm": 2.16174054145813,
      "iter_time": 294.95451522933115,
      "learning_rate": 0.00055,
      "loss": 6.0605,
      "remaining_time": 16222.498337613213,
      "step": 45
    },
    {
      "epoch": 1.08,
      "flops": 308722106.3636528,
      "grad_norm": 2.6286230087280273,
      "iter_time": 289.47133515192115,
      "learning_rate": 0.00054,
      "loss": 5.9606,
      "remaining_time": 15631.452098203743,
      "step": 46
    },
    {
      "epoch": 1.09,
      "flops": 314380902.1826868,
      "grad_norm": 1.9926761388778687,
      "iter_time": 284.2609067521197,
      "learning_rate": 0.00053,
      "loss": 5.8795,
      "remaining_time": 15065.828057862343,
      "step": 47
    },
    {
      "epoch": 1.1,
      "flops": 320037609.4550696,
      "grad_norm": 1.9290679693222046,
      "iter_time": 279.2365574538708,
      "learning_rate": 0.00052,
      "loss": 5.7855,
      "remaining_time": 14520.30098760128,
      "step": 48
    },
    {
      "epoch": 2.01,
      "flops": 325696164.2879029,
      "grad_norm": 1.9485877752304077,
      "iter_time": 274.38517894550245,
      "learning_rate": 0.0005099999999999999,
      "loss": 5.7005,
      "remaining_time": 13993.644126220624,
      "step": 49
    },
    {
      "epoch": 2.02,
      "flops": 331292555.8318823,
      "grad_norm": 1.9524025917053223,
      "iter_time": 269.75010076999666,
      "learning_rate": 0.0005,
      "loss": 5.6148,
      "remaining_time": 13487.505038499834,
      "step": 50
    },
    {
      "epoch": 2.03,
      "flops": 336875818.51767415,
      "grad_norm": 2.249147891998291,
      "iter_time": 265.27935639082216,
      "learning_rate": 0.00049,
      "loss": 5.5466,
      "remaining_time": 12998.688463150285,
      "step": 51
    },
    {
      "epoch": 2.04,
      "flops": 342420773.02625394,
      "grad_norm": 1.6960186958312988,
      "iter_time": 260.98358323940863,
      "learning_rate": 0.00047999999999999996,
      "loss": 5.4801,
      "remaining_time": 12527.211995491614,
      "step": 52
    },
    {
      "epoch": 2.05,
      "flops": 347901974.6741016,
      "grad_norm": 1.6918725967407227,
      "iter_time": 256.871782356838,
      "learning_rate": 0.00047000000000000004,
      "loss": 5.4105,
      "remaining_time": 12072.973770771387,
      "step": 53
    },
    {
      "epoch": 2.06,
      "flops": 353396984.9514366,
      "grad_norm": 1.5538970232009888,
      "iter_time": 252.877653532558,
      "learning_rate": 0.00045999999999999996,
      "loss": 5.3503,
      "remaining_time": 11632.372062497669,
      "step": 54
    },
    {
      "epoch": 2.07,
      "flops": 358842716.99237615,
      "grad_norm": 1.5027669668197632,
      "iter_time": 249.04002809091048,
      "learning_rate": 0.00045,
      "loss": 5.2856,
      "remaining_time": 11206.801264090971,
      "step": 55
    },
    {
      "epoch": 2.08,
      "flops": 364241646.84989256,
      "grad_norm": 1.637515664100647,
      "iter_time": 245.34866095866477,
      "learning_rate": 0.00043999999999999996,
      "loss": 5.2232,
      "remaining_time": 10795.34108218125,
      "step": 56
    },
    {
      "epoch": 2.09,
      "flops": 369619399.9404709,
      "grad_norm": 2.3642001152038574,
      "iter_time": 241.77897679178338,
      "learning_rate": 0.00043,
      "loss": 5.1868,
      "remaining_time": 10396.496002046686,
      "step": 57
    },
    {
      "epoch": 2.1,
      "flops": 374943710.62678605,
      "grad_norm": 1.3705930709838867,
      "iter_time": 238.34564439181625,
      "learning_rate": 0.00042,
      "loss": 5.1556,
      "remaining_time": 10010.517064456282,
      "step": 58
    },
    {
      "epoch": 3.01,
      "flops": 380272305.05755633,
      "grad_norm": 2.488478899002075,
      "iter_time": 235.00580802610364,
      "learning_rate": 0.00041,
      "loss": 5.1139,
      "remaining_time": 9635.238129070249,
      "step": 59
    },
    {
      "epoch": 3.02,
      "flops": 385566606.57964486,
      "grad_norm": 1.517048954963684,
      "iter_time": 231.77889058589935,
      "learning_rate": 0.00039999999999999996,
      "loss": 5.0799,
      "remaining_time": 9271.155623435974,
      "step": 60
    },
    {
      "epoch": 3.03,
      "flops": 390828543.5980526,
      "grad_norm": 1.3373280763626099,
      "iter_time": 228.6583254571821,
      "learning_rate": 0.00039,
      "loss": 5.0414,
      "remaining_time": 8917.674692830102,
      "step": 61
    },
    {
      "epoch": 3.04,
      "flops": 396063722.5922866,
      "grad_norm": 1.2839902639389038,
      "iter_time": 225.63591468334198,
      "learning_rate": 0.00037999999999999997,
      "loss": 5.0092,
      "remaining_time": 8574.164757966995,
      "step": 62
    },
    {
      "epoch": 3.05,
      "flops": 401270500.93708515,
      "grad_norm": 1.5033246278762817,
      "iter_time": 222.70812360067217,
      "learning_rate": 0.00037,
      "loss": 4.9809,
      "remaining_time": 8240.20057322487,
      "step": 63
    },
    {
      "epoch": 3.06,
      "flops": 406448044.1146908,
      "grad_norm": 1.312030553817749,
      "iter_time": 219.87115355581045,
      "learning_rate": 0.00036,
      "loss": 4.9561,
      "remaining_time": 7915.361528009176,
      "step": 64
    },
    {
      "epoch": 3.07,
      "flops": 411589021.5170479,
      "grad_norm": 1.3836473226547241,
      "iter_time": 217.12483970201933,
      "learning_rate": 0.00035,
      "loss": 4.9195,
      "remaining_time": 7599.3693895706765,
      "step": 65
    },
    {
      "epoch": 3.08,
      "flops": 416715281.5070199,
      "grad_norm": 1.3244857788085938,
      "iter_time": 214.45385923168877,
      "learning_rate": 0.00033999999999999997,
      "loss": 4.8945,
      "remaining_time": 7291.4312138774185,
      "step": 66
    },
    {
      "epoch": 3.09,
      "flops": 421803288.5188751,
      "grad_norm": 1.5830367803573608,
      "iter_time": 211.86700709186383,
      "learning_rate": 0.00032999999999999994,
      "loss": 4.8746,
      "remaining_time": 6991.611234031507,
      "step": 67
    },
    {
      "epoch": 3.1,
      "flops": 426860173.23541385,
      "grad_norm": 1.2905100584030151,
      "iter_time": 209.35708206892014,
      "learning_rate": 0.00032,
      "loss": 4.8613,
      "remaining_time": 6699.426626205444,
      "step": 68
    },
    {
      "epoch": 4.01,
      "flops": 431884810.58496004,
      "grad_norm": 1.4537477493286133,
      "iter_time": 206.92137840865314,
      "learning_rate": 0.00031,
      "loss": 4.838,
      "remaining_time": 6414.562730668247,
      "step": 69
    },
    {
      "epoch": 4.02,
      "flops": 436843682.8517934,
      "grad_norm": 1.329351544380188,
      "iter_time": 204.57249086584363,
      "learning_rate": 0.0003,
      "loss": 4.8172,
      "remaining_time": 6137.174725975309,
      "step": 70
    },
    {
      "epoch": 4.03,
      "flops": 436138929.3294134,
      "grad_norm": 2.2686820030212402,
      "iter_time": 204.9030579714708,
      "learning_rate": 0.00029,
      "loss": 4.8019,
      "remaining_time": 5942.188681172653,
      "step": 71
    },
    {
      "epoch": 0.01,
      "flops": 151654209364.9856,
      "grad_norm": 59.71687316894531,
      "iter_time": 0.5892760952313741,
      "learning_rate": 0.00028,
      "loss": 6.0373,
      "remaining_time": 16.499730666478477,
      "step": 72
    },
    {
      "epoch": 0.02,
      "flops": 78034747275.61116,
      "grad_norm": 29.816957473754883,
      "iter_time": 1.1452103510294875,
      "learning_rate": 0.00027,
      "loss": 5.2049,
      "remaining_time": 30.920679477796163,
      "step": 73
    },
    {
      "epoch": 0.03,
      "flops": 53121101056.8581,
      "grad_norm": 11.894564628601074,
      "iter_time": 1.6823107680758915,
      "learning_rate": 0.00026,
      "loss": 4.897,
      "remaining_time": 43.74007996997318,
      "step": 74
    },
    {
      "epoch": 0.04,
      "flops": 40433516819.86928,
      "grad_norm": 2.6827166080474854,
      "iter_time": 2.2102010250091553,
      "learning_rate": 0.00025,
      "loss": 4.8009,
      "remaining_time": 55.25502562522888,
      "step": 75
    },
    {
      "epoch": 0.05,
      "flops": 32714638726.393497,
      "grad_norm": 2.006924867630005,
      "iter_time": 2.7316884367089522,
      "learning_rate": 0.00023999999999999998,
      "loss": 4.7939,
      "remaining_time": 65.56052248101486,
      "step": 76
    },
    {
      "epoch": 0.06,
      "flops": 27655937758.413273,
      "grad_norm": 1.4223616123199463,
      "iter_time": 3.23135671987162,
      "learning_rate": 0.00022999999999999998,
      "loss": 4.7776,
      "remaining_time": 74.32120455704725,
      "step": 77
    },
    {
      "epoch": 0.07,
      "flops": 24012809485.877117,
      "grad_norm": 2.487881898880005,
      "iter_time": 3.721605352866344,
      "learning_rate": 0.00021999999999999998,
      "loss": 4.7633,
      "remaining_time": 81.87531776305957,
      "step": 78
    },
    {
      "epoch": 0.08,
      "flops": 21241088597.04713,
      "grad_norm": 1.674662470817566,
      "iter_time": 4.20723259599903,
      "learning_rate": 0.00021,
      "loss": 4.7496,
      "remaining_time": 88.35188451597963,
      "step": 79
    },
    {
      "epoch": 0.09,
      "flops": 19095256046.3722,
      "grad_norm": 1.4507540464401245,
      "iter_time": 4.680021053552627,
      "learning_rate": 0.00019999999999999998,
      "loss": 4.7408,
      "remaining_time": 93.60042107105255,
      "step": 80
    },
    {
      "epoch": 0.1,
      "flops": 17405423162.7909,
      "grad_norm": 3.9118340015411377,
      "iter_time": 5.13438825842775,
      "learning_rate": 0.00018999999999999998,
      "loss": 4.7487,
      "remaining_time": 97.55337691012724,
      "step": 81
    },
    {
      "epoch": 1.01,
      "flops": 16011197934.18864,
      "grad_norm": 1.4107252359390259,
      "iter_time": 5.581481203800294,
      "learning_rate": 0.00018,
      "loss": 4.7335,
      "remaining_time": 100.4666616684053,
      "step": 82
    },
    {
      "epoch": 1.02,
      "flops": 14793191605.465855,
      "grad_norm": 2.945513963699341,
      "iter_time": 6.041035815893886,
      "learning_rate": 0.00016999999999999999,
      "loss": 4.7137,
      "remaining_time": 102.69760887019606,
      "step": 83
    },
    {
      "epoch": 1.03,
      "flops": 13780861890.27097,
      "grad_norm": 1.3246437311172485,
      "iter_time": 6.484804871536436,
      "learning_rate": 0.00016,
      "loss": 4.7083,
      "remaining_time": 103.75687794458298,
      "step": 84
    },
    {
      "epoch": 1.04,
      "flops": 12902334254.10501,
      "grad_norm": 2.605520009994507,
      "iter_time": 6.926359103707706,
      "learning_rate": 0.00015,
      "loss": 4.7096,
      "remaining_time": 103.89538655561559,
      "step": 85
    },
    {
      "epoch": 1.05,
      "flops": 12153990591.258015,
      "grad_norm": 1.8437520265579224,
      "iter_time": 7.352827834528546,
      "learning_rate": 0.00014,
      "loss": 4.7082,
      "remaining_time": 102.93958968339965,
      "step": 86
    },
    {
      "epoch": 1.06,
      "flops": 11509797281.537409,
      "grad_norm": 0.9025900363922119,
      "iter_time": 7.764359191916455,
      "learning_rate": 0.00013,
      "loss": 4.6921,
      "remaining_time": 100.93666949491391,
      "step": 87
    },
    {
      "epoch": 1.07,
      "flops": 10934438883.054384,
      "grad_norm": 1.3102571964263916,
      "iter_time": 8.172911410981959,
      "learning_rate": 0.00011999999999999999,
      "loss": 4.6798,
      "remaining_time": 98.0749369317835,
      "step": 88
    },
    {
      "epoch": 1.08,
      "flops": 10446962946.175644,
      "grad_norm": 2.8876867294311523,
      "iter_time": 8.554275609134288,
      "learning_rate": 0.00010999999999999999,
      "loss": 4.6705,
      "remaining_time": 94.09703170047717,
      "step": 89
    },
    {
      "epoch": 1.09,
      "flops": 9984113499.12102,
      "grad_norm": 1.2257611751556396,
      "iter_time": 8.950839784410265,
      "learning_rate": 9.999999999999999e-05,
      "loss": 4.6683,
      "remaining_time": 89.50839784410265,
      "step": 90
    },
    {
      "epoch": 1.1,
      "flops": 8995611713.364569,
      "grad_norm": 2.1899752616882324,
      "iter_time": 9.93442171222561,
      "learning_rate": 9e-05,
      "loss": 4.6667,
      "remaining_time": 89.4097954100305,
      "step": 91
    },
    {
      "epoch": 2.01,
      "flops": 2275278206.9400873,
      "grad_norm": 1.442096471786499,
      "iter_time": 39.277043153410375,
      "learning_rate": 8e-05,
      "loss": 4.6662,
      "remaining_time": 314.216345227283,
      "step": 92
    },
    {
      "epoch": 2.02,
      "flops": 1881202838.7446196,
      "grad_norm": 0.8590582013130188,
      "iter_time": 47.50481898041182,
      "learning_rate": 7e-05,
      "loss": 4.6457,
      "remaining_time": 332.53373286288274,
      "step": 93
    },
    {
      "epoch": 2.03,
      "flops": 1882953033.3749754,
      "grad_norm": 1.2185516357421875,
      "iter_time": 47.46066350886162,
      "learning_rate": 5.9999999999999995e-05,
      "loss": 4.6587,
      "remaining_time": 284.76398105316974,
      "step": 94
    },
    {
      "epoch": 2.04,
      "flops": 1884988714.1485791,
      "grad_norm": 1.2262822389602661,
      "iter_time": 47.409408687290394,
      "learning_rate": 4.9999999999999996e-05,
      "loss": 4.6441,
      "remaining_time": 237.047043436452,
      "step": 95
    },
    {
      "epoch": 2.05,
      "flops": 1887581503.764838,
      "grad_norm": 0.7461879849433899,
      "iter_time": 47.34428693105777,
      "learning_rate": 4e-05,
      "loss": 4.6469,
      "remaining_time": 189.3771477242311,
      "step": 96
    },
    {
      "epoch": 2.06,
      "flops": 1889815766.2987013,
      "grad_norm": 0.8798682689666748,
      "iter_time": 47.28831345027255,
      "learning_rate": 2.9999999999999997e-05,
      "loss": 4.6378,
      "remaining_time": 141.86494035081765,
      "step": 97
    },
    {
      "epoch": 2.07,
      "flops": 1891572284.1887283,
      "grad_norm": 0.7258377075195312,
      "iter_time": 47.244401425731425,
      "learning_rate": 2e-05,
      "loss": 4.6376,
      "remaining_time": 94.48880285146285,
      "step": 98
    },
    {
      "epoch": 2.08,
      "flops": 1893477370.5350175,
      "grad_norm": 1.1054010391235352,
      "iter_time": 47.19686736723389,
      "learning_rate": 1e-05,
      "loss": 4.6219,
      "remaining_time": 47.19686736723389,
      "step": 99
    },
    {
      "epoch": 2.09,
      "flops": 1895516639.6216161,
      "grad_norm": 0.8730975389480591,
      "iter_time": 47.14609117746353,
      "learning_rate": 0.0,
      "loss": 4.6316,
      "remaining_time": 0.0,
      "step": 100
    }
  ],
  "logging_steps": 1,
  "max_steps": 100,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 9223372036854775807,
  "save_steps": 60,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 8936620032000.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
